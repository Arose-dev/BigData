{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cookies (probably unimportant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cookies(file_path):\n",
    "  # Load in cookies\n",
    "  cookies_raw = json.load(open(file_path, 'r'))\n",
    "  return {cookie['name']: cookie['value'] for cookie in cookies_raw}\n",
    "\n",
    "def fetch_data(session, url, cookies):\n",
    "  return session.get(url, cookies=cookies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(soup, url):\n",
    "  question_count = 28\n",
    "\n",
    "  responses = [[] for _ in range(question_count)]\n",
    "\n",
    "  # Extract response counts for each question\n",
    "  for i in range(question_count):\n",
    "      question_id = f\"ContentPlaceHolder1_dlQuestionnaire_rptChoices_{i+1}_rbSelect_\"\n",
    "      question_responses = soup.find_all(id=lambda x: x and x.startswith(question_id))\n",
    "      for response in question_responses:\n",
    "        responses[i].append(response.contents[0])\n",
    "  \n",
    "  data = {\"Instructor\": soup.find(id=\"ContentPlaceHolder1_lblInstructorName\").text.replace('\"','').replace(\",\",\"\"),\n",
    "         \"Course\": soup.find(id=\"ContentPlaceHolder1_lblCourseDescription\").text}\n",
    "  \n",
    "  # grades received\n",
    "  table = soup.find('table', id=\"ContentPlaceHolder1_tblGradesReceived\")\n",
    "  grades = []\n",
    "\n",
    "  if table:\n",
    "    for td in table.find('tbody').find('tr').find_all('td'):\n",
    "      grades.append(td.text)\n",
    "\n",
    "  data[\"Grades\"] = \"/\".join(grades)\n",
    "  \n",
    "  # Convert the list of responses into a dictionary with question numbers as keys\n",
    "  end = {f'Question {i+1}': \"/\".join(responses[i]) for i in range(question_count)}\n",
    "\n",
    "  # Add link at end\n",
    "  end['Link'] = url\n",
    "\n",
    "  data.update(end)\n",
    "\n",
    "  return data\n",
    "\n",
    "def fetch_and_parse_data(session, url, cookies, headers):\n",
    "  response = session.get(url, cookies=cookies, headers=headers)\n",
    "  if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.text, 'html.parser')\n",
    "      return parse_data(soup, url)\n",
    "  else:\n",
    "      print(f\"Error fetching {url}: Status code {response.status_code}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(data, file_path):\n",
    "    pd.DataFrame([data]).to_csv(file_path, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  max_attempts = 20  # Set a maximum number of attempts to avoid infinite loops\n",
    "  attempt = 0\n",
    "\n",
    "  while attempt < max_attempts:\n",
    "    try:\n",
    "      cookies = load_cookies('cape.ucsd.edu_cookies.json')\n",
    "      headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82 Safari/537.36',\n",
    "        'Accept-Encoding': '*',\n",
    "        'Connection': 'keep-alive'\n",
    "      }\n",
    "\n",
    "      csv_file_path = 'master.csv'\n",
    "\n",
    "      cur = pd.read_csv(csv_file_path).shape[0]\n",
    "      df = pd.read_csv('capes_data.csv')\n",
    "\n",
    "      session = requests.Session()\n",
    "      session.headers.update(headers)\n",
    "      print(f\"Attempt {attempt+1}: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "      for index in range(cur, 30000):  # Replace 'Link' with the actual column name containing the links\n",
    "        url = df['Evalulation URL'][index]\n",
    "        # only print every 10\n",
    "        if ((index+1) % 10 == 0):\n",
    "          print(f\"{index+1}: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "        response = fetch_data(session, url, cookies)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        append_to_csv(parse_data(soup, url), csv_file_path)\n",
    "      # If this block executes successfully, break out of the loop\n",
    "      attempt = max_attempts  # Ensure the loop exits after successful execution\n",
    "    except Exception as e:\n",
    "        # Handle specific exceptions you're expecting and decide to retry\n",
    "        print(f\"Error encountered: {e}. Attempting to restart...\")\n",
    "        attempt += 1\n",
    "        if attempt == max_attempts:\n",
    "            print(\"Maximum attempts reached. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
